{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10071231,
          "sourceType": "datasetVersion",
          "datasetId": 6207489
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "**Goal:** Democratize the understanding of legislature that flows through congress. <br />\n",
        "\n",
        "**Motivation**: <br />\n",
        "Thousands of bills are introduced by the United States house of representatives or senate every year. From the beginning of the **2023 congressional session** to now, there have been **over 10,000 pieces** of legislature just from the house side alone. These documents, when passed into law have significant impacts on organizations and the people living within the United States. Then surely, it would be in the best interest of the affected groups to be able to access and understand what these effects will be to aid in making informed decisions. However, legislature can be wordy and not the most readable to everyone. Between 2021 and 2024, there were 2 laws passes with roughly **700,000 words** each. From the sheer volume of bills to the length and semantics of the bills, it can be cumbersome or nigh impossible to keep track of all that is happening in congress, even mores so in the age of misinformation. <br />\n",
        "\n",
        "**Idea:** <br/>\n",
        "The adoption and application of large language models (LLMs) has changed the world in several ways and improved efficiency in many contexts where language is concerned. As legislature consists of written language, LLMs are most apropirate to handle the task of helping people understand the impacts of laws. In this notebook, we combine retrieval augmented generation (RAG) with a question and answer framework. The technologies used will be **Google's Gemini LLM** which has proven to be one of top performing models for a broad range of natural language tasks, and LlamaIndex paired with Chroma. Both LlamaIndex and Chroma are open source APIs that assists in performing storage and retrieval of text. You can read more about [Gemini here](https://ai.google.dev/), [LlamaIndex here](https://www.llamaindex.ai/), and [Chromadb here](https://www.trychroma.com/). One key advantage of using **Gemini is its long context window of 2 million tokens** for the pro version and 1 million for the flash version so that it is possible to fit even legislature with hundreds of thousands of words into one request. A second advantage is **context caching** where users can pre-process any data that is at least 32,769 tokens long in the case several queries need to be made in relation to that data point. In effect, saving computational costs and time which in turn alleviates the burden on the environment. <br />\n",
        "\n",
        "**Method:** <br />\n",
        "First, we will pull bills from the congress.gov API. Then, a indexed vector database of the bills will be created for efficient retrieval of relevant documents. Lastly, users will be able to ask questions about any area of law they are invested in. For example, suppose I am a individual concerned about health care laws, then I can submit a query to pull relevant laws from the database, cache those laws, start a chat with Gemini, and ask questions that come to mind as I review the responses. Another example is, suppose I am a property developer and I want to know how legal conditions have changed or may changed. Again, I would be able to type an initial query to retrieve laws related to construction and development, cache those laws, and chat about them."
      ],
      "metadata": {
        "id": "_SSdgPze6Y9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "To begin we need to sign up for Google AI Studio and congress.gov for API keys to access Gemini and congress.gove endpoint so that we can retrieve legislation. Now I have already saved all of the passed laws from 2021 - 2024 in the forms of JSON and LlamaIndex indices. However, if you wish to get legislature that was proposed or legislature from other years then you may do so using the methods defined in the Legislation Retrieval section. Note that congress.gov is free, but they place limits on requests. **You do not have to interact with the congress.gov API as I have already loaded the passed laws for 2021-2024 as a public kaggle dataset.** Unless you want to download laws from other years or not only bills that were signed into law."
      ],
      "metadata": {
        "id": "Ax1cXEMn6Y9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gemini, LlamaIndex, and Chroma API"
      ],
      "metadata": {
        "id": "pZfjFkOd6Y9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-vector-stores-chroma\n",
        "%pip install llama-index-llms-gemini llama-index\n",
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install -U -q \"google-generativeai>=0.8.3\" chromadb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T20:32:37.051909Z",
          "iopub.execute_input": "2024-12-01T20:32:37.052581Z",
          "iopub.status.idle": "2024-12-01T20:33:26.817719Z",
          "shell.execute_reply.started": "2024-12-01T20:32:37.052515Z",
          "shell.execute_reply": "2024-12-01T20:33:26.816442Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "myVwl9OX6Y9B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API Keys\n",
        "- To generate a congress.gov API key go [here to congress.gov](https://www.congress.gov/help/using-data-offsite) and sign up\n",
        "- To generate Google AI Studio API key for Gemini go [here to aistudio.google.com](https://aistudio.google.com/app/apikey) <br />\n",
        "Afterwards you can add API keys to the kaggle notebook by using **\"Add-ons\"** dropdown menu and clicking **\"Secrets\"**."
      ],
      "metadata": {
        "id": "KcNgbz_x6Y9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SET YOU API KEYS IN THE QUOTATION MARKS\n",
        "\n",
        "CONGRESS_API_KEY = \"\"\n",
        "GOOGLE_API_KEY = \"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T20:37:29.389728Z",
          "iopub.execute_input": "2024-12-01T20:37:29.390635Z",
          "iopub.status.idle": "2024-12-01T20:37:30.302781Z",
          "shell.execute_reply.started": "2024-12-01T20:37:29.390593Z",
          "shell.execute_reply": "2024-12-01T20:37:30.30197Z"
        },
        "id": "1J8XAO1T6Y9B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code to Initialize congress.gov API Client"
      ],
      "metadata": {
        "id": "xJC5fjoO6Y9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from typing import List, Union, Optional\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "from urllib.parse import urljoin\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "API_VERSION = \"v3\"\n",
        "ROOT_URL = \"https://api.congress.gov/\"\n",
        "RESPONSE_FORMAT = \"json\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T21:40:38.689281Z",
          "iopub.execute_input": "2024-12-01T21:40:38.68965Z",
          "iopub.status.idle": "2024-12-01T21:40:38.694942Z",
          "shell.execute_reply.started": "2024-12-01T21:40:38.689615Z",
          "shell.execute_reply": "2024-12-01T21:40:38.693947Z"
        },
        "id": "4gF_hW4L6Y9C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly copied from congress.gov github repo: https://github.com/LibraryOfCongress/api.congress.gov\n",
        "class _MethodWrapper:\n",
        "    \"\"\" Wrap request method to facilitate queries.  Supports requests signature. \"\"\"\n",
        "\n",
        "    def __init__(self, parent, http_method):\n",
        "        self._parent = parent\n",
        "        self._method = getattr(parent._session, http_method)\n",
        "\n",
        "    def __call__(self, endpoint, *args, **kwargs):  # full signature passed here\n",
        "        response = self._method(\n",
        "            urljoin(self._parent.base_url, endpoint), *args, **kwargs\n",
        "        )\n",
        "        # unpack\n",
        "        if response.headers.get(\"content-type\", \"\").startswith(\"application/json\"):\n",
        "            return response.json(), response.status_code\n",
        "        else:\n",
        "            return response.content, response.status_code\n",
        "\n",
        "\n",
        "class CDGClient:\n",
        "    \"\"\" A sample client to interface with Congress.gov. \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        api_key,\n",
        "        api_version=API_VERSION,\n",
        "        response_format=RESPONSE_FORMAT,\n",
        "        raise_on_error=True,\n",
        "    ):\n",
        "        self.base_url = urljoin(ROOT_URL, api_version) + \"/\"\n",
        "        self._session = requests.Session()\n",
        "\n",
        "        # do not use url parameters, even if offered, use headers\n",
        "        self._session.params = {\"format\": response_format}\n",
        "        self._session.headers.update({\"x-api-key\": api_key})\n",
        "\n",
        "        if raise_on_error:\n",
        "            self._session.hooks = {\n",
        "                \"response\": lambda r, *args, **kwargs: r.raise_for_status()\n",
        "            }\n",
        "\n",
        "    def __getattr__(self, method_name):\n",
        "        \"\"\"Find the session method dynamically and cache for later.\"\"\"\n",
        "        method = _MethodWrapper(self, method_name)\n",
        "        self.__dict__[method_name] = method\n",
        "        return method\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T20:19:57.755Z",
          "iopub.execute_input": "2024-11-30T20:19:57.755349Z",
          "iopub.status.idle": "2024-11-30T20:19:57.765613Z",
          "shell.execute_reply.started": "2024-11-30T20:19:57.755321Z",
          "shell.execute_reply": "2024-11-30T20:19:57.764294Z"
        },
        "id": "C0wrZYEK6Y9C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the client to interact with to get the texts and legislation metadata.\n",
        "client = CDGClient(CONGRESS_API_KEY)  # pass the key, response_format=\"xml\" if needed"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T15:37:33.508478Z",
          "iopub.status.idle": "2024-12-01T15:37:33.508923Z",
          "shell.execute_reply.started": "2024-12-01T15:37:33.508726Z",
          "shell.execute_reply": "2024-12-01T15:37:33.508748Z"
        },
        "id": "sohSLwuC6Y9C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Congressional Session to Year\n",
        "The subsequent cell is to create a mapping from congressional sessions to years. How congress.gov stores congressional sessions is by number. For example, 2023-2024 is session 118 and the API only returns the session number."
      ],
      "metadata": {
        "id": "ttf7mrQt6Y9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "congress_to_years = {}\n",
        "for i in reversed(range(1, 119)):\n",
        "    start = 1906+i\n",
        "    end = 1905+i\n",
        "    congress_to_years[i] = str(start) + \"-\" + str(end)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:35:33.500547Z",
          "iopub.execute_input": "2024-12-01T13:35:33.50107Z",
          "iopub.status.idle": "2024-12-01T13:35:33.508781Z",
          "shell.execute_reply.started": "2024-12-01T13:35:33.501008Z",
          "shell.execute_reply": "2024-12-01T13:35:33.507182Z"
        },
        "id": "gtEMk7RU6Y9C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval of Bills\n",
        "Run get_laws() and pass in a list of integers that specify which congressional sessions you wish to pull legislature from and the chambers of interest. The only chambers are the house of representatives ('hr') and senate ('s'). Currently the method only supports filtering out unpassed bills or not, but it can be easily modified to include only bills that passed one part of congress or bills that passed both parts of congress. Do keep in mind that congress.gov API **only allows 5,000 requests per hour** and each request can return a max of 250 bills."
      ],
      "metadata": {
        "id": "tfxcn0rn6Y9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bill_dict(bill: dict) -> Union[dict, str]:\n",
        "    \"Create a dictionary to hold the legislation metadata.\"\n",
        "\n",
        "    tmp = {}\n",
        "    tmp['latest_action'] = bill['latestAction']['text']\n",
        "    tmp['num'] = bill['number']\n",
        "    tmp['congress'] = bill['congress']\n",
        "    tmp['latest_action_date'] = bill['latestAction']['actionDate']\n",
        "    tmp['chamber'] = bill['type'].lower()\n",
        "    title = bill['title']\n",
        "\n",
        "    return tmp, title\n",
        "\n",
        "# only updates if the bill has become a law already\n",
        "def update_laws(\n",
        "    bills: dict,\n",
        "    laws: dict,\n",
        "    passed: bool = True,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Update the dict of laws with newly retrieved laws where the keys are titles and values\n",
        "    are law metadata.\n",
        "    \"\"\"\n",
        "    for b in bills:\n",
        "        if passed:\n",
        "            if 'Became Public Law' in b['latestAction']['text']:\n",
        "                tmp, title = create_bill_dict(b)\n",
        "                laws[title] = tmp\n",
        "        else:\n",
        "            tmp, title = create_bill_dict(b)\n",
        "            laws[title] = tmp\n",
        "    return laws\n",
        "\n",
        "# options for chamber are \"hr\" for house and \"s\" for senate\n",
        "# 118 is the congress 2023-2024\n",
        "def get_laws(\n",
        "    congress: List[int] = [118, 117],\n",
        "    chamber: List[str] = ['hr', 's'],\n",
        "    passed: bool = True,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Loop through specified chamber of congress and congressional sessions to pull the associated\n",
        "    bills presented by those chambers and in those years.\n",
        "    \"\"\"\n",
        "    laws = {}\n",
        "    for ch in chamber:\n",
        "        for c in congress:\n",
        "            num_bills_retrieved = 0\n",
        "            endpoint = f\"https://api.congress.gov/v3/bill/{c}/{ch}?offset=0&limit=250&format=json\"\n",
        "            data, _ = client.get(endpoint)\n",
        "            total_num_bills = data['pagination']['count']\n",
        "            next_page = data['pagination']['next']\n",
        "            bills = data['bills']\n",
        "            num_bills_retrieved += len(bills)\n",
        "\n",
        "            laws = update_laws(bills=bills, laws=laws)\n",
        "\n",
        "            while num_bills_retrieved < total_num_bills:\n",
        "\n",
        "                data, _ = client.get(next_page)\n",
        "                bills = data['bills']\n",
        "                num_bills_retrieved += len(bills)\n",
        "                try:\n",
        "                    next_page = data['pagination']['next']\n",
        "                    laws = update_laws(bills=bills, laws=laws, passed=passed)\n",
        "                except KeyError:\n",
        "                    print(\"No next page\")\n",
        "                    print(f\"Num bills retrieved: {num_bills_retrieved} out of {data['pagination']['count']}\")\n",
        "                    break\n",
        "    return laws"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T20:35:23.419705Z",
          "iopub.execute_input": "2024-11-30T20:35:23.420073Z",
          "iopub.status.idle": "2024-11-30T20:35:23.430571Z",
          "shell.execute_reply.started": "2024-11-30T20:35:23.420041Z",
          "shell.execute_reply": "2024-11-30T20:35:23.429526Z"
        },
        "id": "rAO7JwP26Y9C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving law dict w/o text\n",
        "passed_laws = get_laws(congress=[118, 117], chamber=['hr', 's'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T20:35:25.861692Z",
          "iopub.execute_input": "2024-11-30T20:35:25.86206Z",
          "iopub.status.idle": "2024-11-30T20:43:14.784256Z",
          "shell.execute_reply.started": "2024-11-30T20:35:25.862029Z",
          "shell.execute_reply": "2024-11-30T20:43:14.782902Z"
        },
        "id": "d0fOvwy56Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "titles = list(passed_laws.keys())\n",
        "num_laws_pulled = len(titles)\n",
        "print(f\"Num laws passed in last two years from both chambers of congress: {num_laws_pulled}\")\n",
        "print(f\"Name of first retrieved law: {titles[0]}\")\n",
        "print(f\"Example of first law from dict: {passed_laws[titles[0]]}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T20:52:48.309381Z",
          "iopub.execute_input": "2024-11-30T20:52:48.309864Z",
          "iopub.status.idle": "2024-11-30T20:52:48.316271Z",
          "shell.execute_reply.started": "2024-11-30T20:52:48.309823Z",
          "shell.execute_reply": "2024-11-30T20:52:48.314951Z"
        },
        "id": "38Aj-h8K6Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# save file so as not to need to get from API again\n",
        "with open(\"passed_laws_117_118_both_chambers.json\", \"w\") as fout:\n",
        "    json.dump(passed_laws, fout)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T20:46:41.447451Z",
          "iopub.execute_input": "2024-11-30T20:46:41.447883Z",
          "iopub.status.idle": "2024-11-30T20:46:41.459401Z",
          "shell.execute_reply.started": "2024-11-30T20:46:41.447809Z",
          "shell.execute_reply": "2024-11-30T20:46:41.458114Z"
        },
        "id": "YWdLaegf6Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG with LlamaIndex and Chroma\n",
        "The items returned above from congress.gov API only contains the legislature metadata from which we can use to again request the API to get links to html, xml, pdf versions of the text. Getting the acutal texts requires using requests to download the html which is the most friendly format to work with for inputting to an LLM without much pre-processing. Additionally, to store the legislature, we need to turn each one into a Document class object used by LlamaIndex that will later be transformed into nodes, embedded, and stored in a Chroma database instance."
      ],
      "metadata": {
        "id": "CqyFeRzJ6Y9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage\n",
        "from llama_index.core.schema import MetadataMode, TextNode\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "import chromadb\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "from google.api_core import retry"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T20:37:41.12189Z",
          "iopub.execute_input": "2024-12-01T20:37:41.122789Z",
          "iopub.status.idle": "2024-12-01T20:37:59.830662Z",
          "shell.execute_reply.started": "2024-12-01T20:37:41.122748Z",
          "shell.execute_reply": "2024-12-01T20:37:59.829829Z"
        },
        "id": "6Uf-B41w6Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving The Text Associated with Each Law\n",
        "The methods in the next cell request the URL for the legislative texts and turns each legislative dictionary into a Document for further processing by LlamaIndex and Chroma."
      ],
      "metadata": {
        "id": "teoBVy-86Y9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_law_doc(\n",
        "    name: str,\n",
        "    law: dict,\n",
        "    pattern\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Download the html text of the speficied legislature and clean it by removing html tags,\n",
        "    underscores that act as separators, additional white space, and new lines.\n",
        "    \"\"\"\n",
        "    # remove the underscores and newlines and spaces to save on token usage\n",
        "    def req_and_clean_text(url: str) -> str:\n",
        "\n",
        "        r = requests.get(url)\n",
        "        text = r.text\n",
        "        # remove html tags\n",
        "        cleantext = re.sub(pattern, '', text)\n",
        "        # remove underscore separators\n",
        "        cleantext = re.sub(\"_\", \"\", cleantext)\n",
        "        # replace extra white space with single space\n",
        "        cleantext = re.sub(' +', ' ', cleantext)\n",
        "        # replace newline with space\n",
        "        cleantext = re.sub('\\n', ' ', cleantext)\n",
        "        return cleantext\n",
        "\n",
        "    BILL_PATH = \"bill\"\n",
        "    CONGRESS = law['congress']\n",
        "    CH = law['chamber']\n",
        "    BILL_NUM = law['num']\n",
        "    endpoint = f\"{BILL_PATH}/{CONGRESS}/{CH}/{BILL_NUM}/text\"\n",
        "    data, _ = client.get(endpoint)\n",
        "    text = data['textVersions'][0]['formats'][0] # assuming formatted txt always first\n",
        "    if text['type'] == \"Formatted Text\": # this is formatted html\n",
        "        url = text['url']\n",
        "        return req_and_clean_text(url=url)\n",
        "    return None\n",
        "\n",
        "# issue is that we might have too much txt to store on ram so we want to chunk into 5000\n",
        "# not an issue for passed laws in 117 & 118 congress so ignore for now\n",
        "# create and return documents here\n",
        "def create_documents(\n",
        "    laws: dict,\n",
        "    save_laws: bool,\n",
        "    out_dir: str,\n",
        "    law_file_name: str\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Request and turn legislature dicts into documents. Option to save the dict of legislature\n",
        "    with text to a json file.\n",
        "    \"\"\"\n",
        "    # as per recommendation from @freylis, compile once only\n",
        "    CLEANR = re.compile('<.*?>') # remove html tags\n",
        "    docs = []\n",
        "    for n, d in laws.items():\n",
        "        txt = get_law_doc(name=n, law=d, pattern=CLEANR)\n",
        "        if txt is None:\n",
        "            del laws[n]\n",
        "        else:\n",
        "            d['doc'] = txt\n",
        "            # llamaindex document creation to append to all docs for vector db\n",
        "            document = Document(\n",
        "                text=txt,\n",
        "                metadata={\n",
        "                    \"index_id\": str(d[\"congress\"]) + \"-\" + d[\"chamber\"] + \"-\" + str(d[\"num\"]),\n",
        "                    \"title\": n,\n",
        "                    \"congress\": d[\"congress\"],\n",
        "                    \"chamber\": d[\"chamber\"],\n",
        "                    \"bill_num\": d[\"num\"],\n",
        "                    \"latest_action_date\": d[\"latest_action_date\"],\n",
        "                    \"lates_action\": d['latest_action']\n",
        "                },\n",
        "                excluded_llm_metadata_keys=[\"index_id\", \"congress\", \"chamber\", \"bill_num\", \"latest_action_date\"],\n",
        "                metadata_seperator=\"::\",\n",
        "                metadata_template=\"{key}=>{value}\",\n",
        "                text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n",
        "            )\n",
        "            docs.append(document)\n",
        "\n",
        "    # saving laws as json to directory\n",
        "    if save_laws:\n",
        "        with open(os.path.join(out_dir, f\"{law_file_name}.json\"), 'w') as fout:\n",
        "            json.dump(laws, fout)\n",
        "\n",
        "    return docs\n",
        "\n",
        "def dict_to_docs(laws: dict) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Input a dict of laws with titles as keys and values as dicts of metadata and text to get\n",
        "    a list of LlamaIndex Document class items.\n",
        "    \"\"\"\n",
        "    laws_docs = []\n",
        "    for n, d in laws.items():\n",
        "        document = Document(\n",
        "                    text=d['doc'],\n",
        "                    metadata={\n",
        "                        \"index_id\": str(d[\"congress\"]) + \"-\" + d[\"chamber\"] + \"-\" + str(d[\"num\"]),\n",
        "                        \"title\": n,\n",
        "                        \"congress\": d[\"congress\"],\n",
        "                        \"chamber\": d[\"chamber\"],\n",
        "                        \"bill_num\": d[\"num\"],\n",
        "                        \"latest_action_date\": d[\"latest_action_date\"],\n",
        "                        #\"lates_action\": d['latest_action']\n",
        "                    },\n",
        "                    excluded_llm_metadata_keys=[\"index_id\", \"congress\", \"chamber\", \"bill_num\", \"latest_action_date\"],\n",
        "                    metadata_seperator=\"::\",\n",
        "                    metadata_template=\"{key}=>{value}\",\n",
        "                    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n",
        "                )\n",
        "        laws_docs.append(document)\n",
        "\n",
        "    return laws_docs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T21:45:41.596706Z",
          "iopub.execute_input": "2024-12-01T21:45:41.597459Z",
          "iopub.status.idle": "2024-12-01T21:45:41.612494Z",
          "shell.execute_reply.started": "2024-12-01T21:45:41.597418Z",
          "shell.execute_reply": "2024-12-01T21:45:41.611519Z"
        },
        "id": "sD_m9J0r6Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating documents w/ text of laws retrieved from laws dict\n",
        "\n",
        "# passed_laws_docs = create_documents(\n",
        "#     laws=passed_laws,\n",
        "#     save_laws=True,\n",
        "#     out_dir=\"/kaggle/working\",\n",
        "#     law_file_name=\"passed_laws_117_118_both_w_txt\"\n",
        "# )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T21:24:04.59441Z",
          "iopub.execute_input": "2024-11-30T21:24:04.594987Z",
          "iopub.status.idle": "2024-11-30T21:24:09.500171Z",
          "shell.execute_reply.started": "2024-11-30T21:24:04.594917Z",
          "shell.execute_reply": "2024-11-30T21:24:09.498399Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "6CVB69tL6Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# loading bills saved to disk as a dict\n",
        "LAW_TEXT_PATH = \"\"\n",
        "\n",
        "with open(LAW_TEXT_PATH, \"r\") as fop:\n",
        "    passed_laws = json.load(fop)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T23:05:17.285262Z",
          "iopub.execute_input": "2024-12-01T23:05:17.286329Z",
          "iopub.status.idle": "2024-12-01T23:05:17.699124Z",
          "shell.execute_reply.started": "2024-12-01T23:05:17.286272Z",
          "shell.execute_reply": "2024-12-01T23:05:17.697943Z"
        },
        "id": "WMNsMpO46Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "passed_laws_docs = dict_to_docs(laws=passed_laws)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T17:20:00.838033Z",
          "iopub.execute_input": "2024-12-01T17:20:00.838435Z",
          "iopub.status.idle": "2024-12-01T17:20:00.87032Z",
          "shell.execute_reply.started": "2024-12-01T17:20:00.838401Z",
          "shell.execute_reply": "2024-12-01T17:20:00.868316Z"
        },
        "id": "zK1e-4mZ6Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "law_lengths = []\n",
        "for n, d in passed_laws.items():\n",
        "    law_lengths.append(len(d['doc'].split(\" \")))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T01:58:29.408397Z",
          "iopub.execute_input": "2024-12-01T01:58:29.40922Z",
          "iopub.status.idle": "2024-12-01T01:58:29.793451Z",
          "shell.execute_reply.started": "2024-12-01T01:58:29.409187Z",
          "shell.execute_reply": "2024-12-01T01:58:29.792487Z"
        },
        "id": "jLsfpcEN6Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "avg_law_length = sum(law_lengths) / len(law_lengths)\n",
        "\n",
        "print(f\"Average law length by number of words: {avg_law_length}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T01:58:31.696099Z",
          "iopub.execute_input": "2024-12-01T01:58:31.696441Z",
          "iopub.status.idle": "2024-12-01T01:58:31.701472Z",
          "shell.execute_reply.started": "2024-12-01T01:58:31.69641Z",
          "shell.execute_reply": "2024-12-01T01:58:31.70061Z"
        },
        "id": "2xYUSLwd6Y9D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Chroma Vector DB and LlamaIndex Index\n",
        "The first step in creating a LlamaIndex for RAG is to initialize a storage instance using Chroma. Once the vector store is specified, use VectorStoreIndex class from LlamaIndex and pass in a list of Documents. There is also the requirment to select a model to embed the text and in this case we will be using BAAI's BGE 1.5 large hosted on huggingface as it is a open source model that has shown to perform well according the huggingface's MTEB leaderboard that ranks embedding models. Unfortunately, LlamIndex isn't completely integrated with Google's AI Studio and a embedding model must be given to create a vector index store whereas Google's AI Studio only has a method to return embeddings. Find out more about BAAI BGE [here at huggingface](https://huggingface.co/BAAI/bge-large-en-v1.5)."
      ],
      "metadata": {
        "id": "OID0d8886Y9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector db and index from docs\n",
        "def create_and_save_index(\n",
        "    docs: Union[Document, TextNode],\n",
        "    huggingface_model: str,\n",
        "    db_name: str = \"passed_laws\",\n",
        "    db_path: str = \"./passed_laws_db\",\n",
        "    persist_idx_dir: str = \"./passed_laws_index\",\n",
        "    save_idx: bool = True\n",
        ") -> VectorStoreIndex:\n",
        "\n",
        "    embed_model = HuggingFaceEmbedding(model_name=huggingface_model)\n",
        "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
        "    if isinstance(docs[0], Document):\n",
        "        #embed_fn = GeminiEmbeddingFunction()\n",
        "        #embed_fn.document_mode = True\n",
        "        chroma_collection = chroma_client.get_or_create_collection(\n",
        "            name=db_name,\n",
        "            #embedding_function=embed_fn\n",
        "        )\n",
        "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "        index = VectorStoreIndex.from_documents(\n",
        "            docs, storage_context=storage_context, embed_model=embed_model\n",
        "        )\n",
        "    else:\n",
        "        chroma_client = chromadb.PersistentClient(path=db_path)\n",
        "        chroma_collection = chroma_client.create_collection(name=db_name)\n",
        "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "        index = VectorStoreIndex(\n",
        "            objects=docs, storage_context=storage_context, embed_model=embed_model\n",
        "        )\n",
        "\n",
        "    # persist index to disk\n",
        "    if save_idx:\n",
        "        index.storage_context.persist(persist_dir=persist_idx_dir)\n",
        "\n",
        "    return index"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:37:43.872703Z",
          "iopub.execute_input": "2024-12-01T13:37:43.873108Z",
          "iopub.status.idle": "2024-12-01T13:37:43.884295Z",
          "shell.execute_reply.started": "2024-12-01T13:37:43.873071Z",
          "shell.execute_reply": "2024-12-01T13:37:43.88296Z"
        },
        "id": "QA43gRzx6Y9E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Index\n",
        "\n",
        "Uncomment the next cell with GPU to embed and create the document index."
      ],
      "metadata": {
        "id": "JtlVy9-w6Y9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and saving source documents into vector db\n",
        "\n",
        "passed_laws_index = create_and_save_index(\n",
        "    docs=passed_laws_docs,\n",
        "    huggingface_model=\"BAAI/bge-large-en-v1.5\",\n",
        "    db_name=\"passed_laws_117_118_both\",\n",
        "    db_path=\"./passed_laws_117_118_both_db\",\n",
        "    persist_idx_dir=\"./passed_laws_117_118_both_index\",\n",
        "    save_idx=True\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T00:29:49.05487Z",
          "iopub.execute_input": "2024-12-01T00:29:49.055436Z",
          "iopub.status.idle": "2024-12-01T00:47:12.859125Z",
          "shell.execute_reply.started": "2024-12-01T00:29:49.055403Z",
          "shell.execute_reply": "2024-12-01T00:47:12.858412Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "3lgdIUMS6Y9E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# load document index from disk\n",
        "# SAVED_LAW_DB = \"\"\n",
        "\n",
        "# db2 = chromadb.PersistentClient(path=SAVED_LAW_DB)\n",
        "# chroma_collection = db2.get_or_create_collection(\"passed_laws_117_118_both\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
        "# passed_laws_index = index = VectorStoreIndex.from_vector_store(\n",
        "#     vector_store,\n",
        "#     llm='local',\n",
        "#     embed_model=embed_model,\n",
        "# )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T22:35:59.015306Z",
          "iopub.execute_input": "2024-12-01T22:35:59.016518Z",
          "iopub.status.idle": "2024-12-01T22:35:59.320945Z",
          "shell.execute_reply.started": "2024-12-01T22:35:59.016472Z",
          "shell.execute_reply": "2024-12-01T22:35:59.319383Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "mag_TsC56Y9E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary Index for Structured Hierarchical Retrieval\n",
        "To enhance retrieval of relevant documents we employ what LlamaIndex refers to as [structured hierarchical retrieval](https://docs.llamaindex.ai/en/stable/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval/)(link to LlamaIndex tutorial). The core idea is to use a LLM to create summaries of each document, embed the summaries and form a summary index. Then a retriever is created with the summary index along with the original document index and query strings are matched against the summaries instead of the entire document. Yet, the document will still be pulled as nodes for answering. The advantage of this method is that documents can be immensely long, longer than even Gemini's context window allows for, and that affects the quality of the matches. When documents are too long they usually have to be chunked and that leads to the issue of having to find the similarity of the query with each chunk, and then findind the original parent document and pulling that."
      ],
      "metadata": {
        "id": "oZKUKu6R6Y9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core.schema import IndexNode\n",
        "from llama_index.core.vector_stores import (\n",
        "    FilterOperator,\n",
        "    MetadataFilter,\n",
        "    MetadataFilters,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Sjvd11mP6Y9E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T20:38:35.435924Z",
          "iopub.execute_input": "2024-12-01T20:38:35.436684Z",
          "iopub.status.idle": "2024-12-01T20:38:35.441002Z",
          "shell.execute_reply.started": "2024-12-01T20:38:35.436643Z",
          "shell.execute_reply": "2024-12-01T20:38:35.440001Z"
        },
        "id": "A_7DC3J46Y9E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary Index\n",
        "Here we are leveraging Gemini's long context window to create summaries. The majority of the legislative text are able to be summarized without chunking. It is important to note there are also limits induced by Google AI Studio's API. Safeguards have been built into the methods in the next cell to account for these limits As of now any document that is too much for Gemini's context window. Regardless here are the limits of the **free tiers** for future use:\n",
        "- Flash:\n",
        "    - 1500 requests/day\n",
        "    - 15 requests/minute\n",
        "    - 1,000,000 tokens/minute\n",
        "- Pro:\n",
        "    - 50 requests/day\n",
        "    - 2 requests/minute\n",
        "    - 32,000 tokens/minute"
      ],
      "metadata": {
        "id": "saJQycq-6Y9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_doc(doc, doc_index, include_summary: bool = True):\n",
        "    \"\"\"Process doc.\"\"\"\n",
        "    new_metadata = doc.metadata\n",
        "    doc.text = \" \".join([t for t in doc.text.split(\" \") if t != ''])\n",
        "    print(f\"Length of text after join: {len(doc.text.split(' '))}\")\n",
        "    # now extract out summary\n",
        "    summary_index = SummaryIndex.from_documents([doc])\n",
        "    query_str = \"Give a concise summary of this text in five sentences.\"\n",
        "    if len(doc.text.split(\" \")) > 100_000:\n",
        "        query_engine = summary_index.as_query_engine(\n",
        "            llm=Gemini(model=\"models/gemini-1.5-pro\")\n",
        "        )\n",
        "    else:\n",
        "        query_engine = summary_index.as_query_engine(\n",
        "            llm=Gemini(model=\"models/gemini-1.5-flash\")\n",
        "        )\n",
        "    try:\n",
        "        summary_txt = query_engine.query(query_str)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "    summary_txt = str(summary_txt)\n",
        "\n",
        "    index_id = doc.metadata[\"index_id\"]\n",
        "    # filter for the specific doc id\n",
        "    filters = MetadataFilters(\n",
        "        filters=[\n",
        "            MetadataFilter(\n",
        "                key=\"index_id\", operator=FilterOperator.EQ, value=index_id\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    # might get an error here due to doc.id_\n",
        "    # create an index node using the summary text\n",
        "    try:\n",
        "        index_node = IndexNode(\n",
        "            text=summary_txt,\n",
        "            metadata=new_metadata,\n",
        "            obj=doc_index.as_retriever(filters=filters),\n",
        "            index_id=doc.id_,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Tried creating index_node w/ exception: {e}\")\n",
        "\n",
        "    return index_node\n",
        "\n",
        "\n",
        "def process_docs(docs, doc_index):\n",
        "    \"\"\"Process metadata on docs.\"\"\"\n",
        "\n",
        "    index_nodes = []\n",
        "    tokens_processed = 0\n",
        "    #model = genai.GenerativeModel(model_name='gemini-1.5-flash-002')\n",
        "    for i, doc in enumerate(docs):\n",
        "        print(f\"document {i} in passed list\")\n",
        "        # limit is 15 requests per minute for gemini flash and 1 mil tokens per minute\n",
        "        # every 4 characters is a token according to google gemini, so set a low num tokens\n",
        "        tokens_processed += len(doc.text.split(\" \"))\n",
        "        print(f\"Number of words: {len(doc.text.split(' '))}\")\n",
        "        if tokens_processed % 200_000 == 0:\n",
        "            time.sleep(70)\n",
        "        if i % 10 == 0:\n",
        "            time.sleep(70)\n",
        "        node = process_doc(doc, doc_index=doc_index)\n",
        "        if node:\n",
        "            index_nodes.append(node)\n",
        "\n",
        "    return index_nodes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T17:28:51.083216Z",
          "iopub.execute_input": "2024-12-01T17:28:51.083598Z",
          "iopub.status.idle": "2024-12-01T17:28:51.094948Z",
          "shell.execute_reply.started": "2024-12-01T17:28:51.083564Z",
          "shell.execute_reply": "2024-12-01T17:28:51.093852Z"
        },
        "id": "9FFxnrFs6Y9E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Summary Index Nodes\n",
        "Uncomment and run the next two nodes."
      ],
      "metadata": {
        "id": "hPPyoNFC6Y9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_index_nodes = process_docs(passed_laws_docs, passed_laws_index)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T17:29:52.721644Z",
          "iopub.execute_input": "2024-12-01T17:29:52.72219Z",
          "iopub.status.idle": "2024-12-01T18:37:19.942051Z",
          "shell.execute_reply.started": "2024-12-01T17:29:52.72214Z",
          "shell.execute_reply": "2024-12-01T18:37:19.940719Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Mbhn9q0U6Y9E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# create summary index from nodes\n",
        "\n",
        "summarized_law_index = create_and_save_index(\n",
        "    docs=summary_index_nodes,\n",
        "    huggingface_model=\"BAAI/bge-large-en-v1.5\",\n",
        "    db_name=\"summary_passed_laws\",\n",
        "    db_path=\"./summary_passed_laws_db\",\n",
        "    persist_idx_dir=\"./summary_passed_laws_index\",\n",
        "    save_idx=True\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T18:40:54.135257Z",
          "iopub.execute_input": "2024-12-01T18:40:54.135698Z",
          "iopub.status.idle": "2024-12-01T18:48:18.152613Z",
          "shell.execute_reply.started": "2024-12-01T18:40:54.135662Z",
          "shell.execute_reply": "2024-12-01T18:48:18.151452Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "EcZyZTGO6Y9I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# load summary index from disk\n",
        "\n",
        "# SUMMARY_DB_PATH = \"\"\n",
        "\n",
        "# db3 = chromadb.PersistentClient(path=SUMMARY_DB_PATH)\n",
        "# chroma_collection = db3.get_or_create_collection(\"summary_passed_laws\")\n",
        "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
        "# summarized_law_index = index = VectorStoreIndex.from_vector_store(\n",
        "#     vector_store,\n",
        "#     llm='local',\n",
        "#     embed_model=embed_model,\n",
        "# )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T20:38:44.531927Z",
          "iopub.execute_input": "2024-12-01T20:38:44.532304Z",
          "iopub.status.idle": "2024-12-01T20:38:47.278435Z",
          "shell.execute_reply.started": "2024-12-01T20:38:44.532268Z",
          "shell.execute_reply": "2024-12-01T20:38:47.277539Z"
        },
        "id": "ci4VLtxA6Y9I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Store Info\n",
        "This class can be passed to a retriever created from a index so that LLMs can use document metadata to assist in more accurate retrieval."
      ],
      "metadata": {
        "id": "XrBI_miO6Y9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# schema setup\n",
        "from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n",
        "\n",
        "vector_store_info = VectorStoreInfo(\n",
        "    content_info=\"Passed Laws\",\n",
        "    metadata_info=[\n",
        "        MetadataInfo(\n",
        "            name=\"title\",\n",
        "            description=\"Name of the bill.\",\n",
        "            type=\"string\",\n",
        "        ),\n",
        "        MetadataInfo(\n",
        "            name=\"congress\",\n",
        "            description=\"Number of the congressional session that relates to year.\",\n",
        "            type=\"integer\",\n",
        "        ),\n",
        "        MetadataInfo(\n",
        "            name=\"chamber\",\n",
        "            description=\"Congressional body that introduced the bill. Either House or Senate.\",\n",
        "            type=\"string\",\n",
        "        ),\n",
        "        MetadataInfo(\n",
        "            name=\"bill_num\",\n",
        "            description=\"The number assigned to the bill.\",\n",
        "            type=\"integer\",\n",
        "        ),\n",
        "        MetadataInfo(\n",
        "            name=\"latest_action_date\",\n",
        "            description=\"Date of most recent action on the bill.\",\n",
        "            type=\"string\",\n",
        "        ),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T21:25:07.565787Z",
          "iopub.execute_input": "2024-12-01T21:25:07.566294Z",
          "iopub.status.idle": "2024-12-01T21:25:07.571874Z",
          "shell.execute_reply.started": "2024-12-01T21:25:07.566253Z",
          "shell.execute_reply": "2024-12-01T21:25:07.570923Z"
        },
        "id": "olCf6wvr6Y9I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Legislature Agent\n",
        "Finally we can assemble all of the pieces into a Gemini LLM agent that can answer questions about laws. When we do retrieval it is necessary to set the top_k parameter that controls the number of documents returned. LlamaIndex also implements post-processors enabling the ability to further filter out documents. For our use case we will set the top_k to be large and post-process documents by making sure they have a similarity score of more than 0.2."
      ],
      "metadata": {
        "id": "uEDQhfzV6Y9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
        "from llama_index.core import QueryBundle, Settings\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.response.notebook_utils import display_source_node"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T20:38:50.94086Z",
          "iopub.execute_input": "2024-12-01T20:38:50.94165Z",
          "iopub.status.idle": "2024-12-01T20:38:50.951076Z",
          "shell.execute_reply.started": "2024-12-01T20:38:50.94161Z",
          "shell.execute_reply": "2024-12-01T20:38:50.950137Z"
        },
        "id": "aKTEJycA6Y9J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
        "Settings.llm = None\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T20:38:58.902008Z",
          "iopub.execute_input": "2024-12-01T20:38:58.903029Z",
          "iopub.status.idle": "2024-12-01T20:38:58.90974Z",
          "shell.execute_reply.started": "2024-12-01T20:38:58.902986Z",
          "shell.execute_reply": "2024-12-01T20:38:58.908829Z"
        },
        "id": "KkHSKTLp6Y9J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Workflow\n",
        "- Init agent and pass either \"flash\" or \"pro\" as model str. Also, pass in the document index, summary index, vector_store_info, and path to dict of laws with text.\n",
        "- You can ask for a summary by calling the summary method. If the returned text are long enough, it will automatically use context caching, otherwise it is a one-off query.\n",
        "- If you want a longer session, then use start_chat() with a query to match laws to and then you can repeatedly prompt the agent using the ask() method. If a chat was not started, then ask() will be a one-off query."
      ],
      "metadata": {
        "id": "Pn03Vv-n6Y9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LegislatureAgent(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: str,\n",
        "                 legislature_index: VectorStoreIndex,\n",
        "                 summary_index: VectorStoreIndex,\n",
        "                 vector_store_info: VectorStoreInfo,\n",
        "                 law_dict_path: str = \"/kaggle/input/passed-legislation-117-118/passed_laws_117_118_both_w_txt.json\",\n",
        "                ):\n",
        "\n",
        "        # for context caching only able to use stable versions, hence need to specify 00x.\n",
        "        if model == \"flash\":\n",
        "            self.model = \"gemini-1.5-flash-002\"\n",
        "        elif model == \"pro\":\n",
        "            self.model = \"gemini-1.5-pro-002\"\n",
        "        else:\n",
        "            print(\"Model not supported. Defaulting to free tier, flash.\")\n",
        "            self.model = \"gemini-1.5-flash-002\"\n",
        "\n",
        "        self.document_index = legislature_index\n",
        "        self.summary_index = summary_index\n",
        "        self.retriever = VectorIndexAutoRetriever(\n",
        "            summary_index,\n",
        "            vector_store_info=vector_store_info,\n",
        "            similarity_top_k=1000, # set this high to include as many as possible in case of get all\n",
        "            empty_query_top_k=1000,  # if only metadata filters are specified, this is the limit\n",
        "            verbose=True,\n",
        "        )\n",
        "        with open(law_dict_path, \"r\") as fop:\n",
        "            laws = json.load(fop)\n",
        "        self.law_docs = dict_to_docs(laws=laws)\n",
        "        self.chat = None\n",
        "        self.chat_len = None\n",
        "        self.chat_start_time = None\n",
        "        self.agent = None\n",
        "        self.cache = None\n",
        "\n",
        "    def start_chat(\n",
        "        self,\n",
        "        about: str,\n",
        "        chat_len: int, #minutes\n",
        "        temp: float = 0.1,\n",
        "        sim_cutoff: Optional[float] = None\n",
        "    ):\n",
        "        \"Initiate chat session.\"\n",
        "        self.chat_len = chat_len\n",
        "        summary_nodes = self._retrieve_summaries(query=about, sim_cutoff=sim_cutoff)\n",
        "        related_docs, total_len, too_long = self._retrieve_original_doc(nodes=summary_nodes)\n",
        "        # need to turn law nodes into text docs\n",
        "        #law_texts, too_long, total_len = self._node_to_text(nodes=law_nodes, return_too_long=True)\n",
        "        if too_long:\n",
        "            print(f\"Returned texts exceed token limit for {self.model} w/ {total_len} tokens.\")\n",
        "            return None\n",
        "        self.chat_start_time = time.time()\n",
        "        # onlay able to cache if num tokens greater than or equal to 32,769\n",
        "        self._init_agent(total_len, about, temp, related_docs, chat_len)\n",
        "        self.chat = self.agent.start_chat(history=[])\n",
        "\n",
        "    def ask(\n",
        "        self,\n",
        "        about: str,\n",
        "        # temp: float = 0.2,\n",
        "        # sim_cutoff: Optional[float] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        If there was a chat started, then use chat to continue, else init new model and ask a\n",
        "        standalone question.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.chat:\n",
        "            time_since_chat_start = (time.time() - self.chat_start_time) / 60.0\n",
        "            if time_since_chat_start >= self.chat_len:\n",
        "                print(\"Chat time expired based on context cache. Please start a new chat session.\")\n",
        "                self._reset_chat()\n",
        "                return None\n",
        "            print(f\"Time spent since chat start: {0:.2f}\".format(time_since_chat_start))\n",
        "            response = self.chat.send_message(about)\n",
        "            print(response.text)\n",
        "        else:\n",
        "            response = self.agent.generate_content(about)\n",
        "            print(response.text)\n",
        "\n",
        "    def summary(\n",
        "        self,\n",
        "        about: str,\n",
        "        chat_len: int,\n",
        "        temp: float = 0.1,\n",
        "        num_sentences: int = 5,\n",
        "        sim_cutoff: Optional[float] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Putting all the laws into Gemini at once to create a single summary.\n",
        "        \"\"\"\n",
        "        summary_nodes = self._retrieve_summaries(query=about, sim_cutoff=sim_cutoff)\n",
        "        # need to turn law nodes into text docs\n",
        "        law_texts, too_long, total_len = self._node_to_text(\n",
        "            nodes=summary_nodes,\n",
        "            return_too_long=True\n",
        "        )\n",
        "        if too_long:\n",
        "            print(f\"Returned texts exceed token limit for {self.model} w/ {total_len} tokens.\")\n",
        "            return None\n",
        "\n",
        "        self._init_agent(\n",
        "            total_len=total_len,\n",
        "            about=about,\n",
        "            temp=temp,\n",
        "            text_content=law_texts,\n",
        "            chat_len=chat_len)\n",
        "\n",
        "        if self.cache:\n",
        "            response = self.agent.generate_content(\n",
        "                f\"Give a {num_sentences} summary of the text in cached content.\"\n",
        "            )\n",
        "        else:\n",
        "            law_texts.insert(0, f\"Give a {num_sentences} summary of the given text.\")\n",
        "            response = self.agent.generate_content(\n",
        "                law_texts\n",
        "            )\n",
        "        print(response.text)\n",
        "\n",
        "\n",
        "    def _init_agent(\n",
        "        self,\n",
        "        total_len: int,\n",
        "        about: str,\n",
        "        temp: float,\n",
        "        text_content: List[str],\n",
        "        chat_len: int\n",
        "    ):\n",
        "        \"Inits agent based on context caching or not.\"\n",
        "        # onlay able to cache if num tokens greater than or equal to 32,769\n",
        "        if total_len >= 20_000:\n",
        "            self.cache = genai.caching.CachedContent.create(\n",
        "                model=self.model,\n",
        "                display_name=f'{about}', # used to identify the cache\n",
        "                system_instruction=(\n",
        "                    'You are an expert in US legislature, and your job is to answer '\n",
        "                    'the user\\'s query based on the text file you have access to.'\n",
        "                ),\n",
        "                # generation_config=genai.GenerationConfig(\n",
        "                #     temperature=temp,\n",
        "                # ),\n",
        "                contents=text_content,\n",
        "                ttl=datetime.timedelta(minutes=chat_len),\n",
        "            )\n",
        "            model = genai.GenerativeModel.from_cached_content(cached_content=self.cache)\n",
        "        else:\n",
        "            model = genai.GenerativeModel(\n",
        "                self.model,\n",
        "                system_instruction=(\n",
        "                    'You are an expert in US legislature, and your job is to answer '\n",
        "                    'the user\\'s query based on the text file you have access to.'\n",
        "                ),\n",
        "                generation_config=genai.GenerationConfig(\n",
        "                    temperature=temp,\n",
        "                )\n",
        "            )\n",
        "        self.agent = model\n",
        "\n",
        "    def _retrieve_summaries(self, query: str, sim_cutoff: Optional[float] = None):\n",
        "        \"\"\"\n",
        "        Returns nodes of documents using cosine similarity based on summary index summaries compared\n",
        "        to the query.\n",
        "        \"\"\"\n",
        "\n",
        "        nodes = self.retriever.retrieve(QueryBundle(query))\n",
        "        if sim_cutoff:\n",
        "            postprocessor = SimilarityPostprocessor(similarity_cutoff=sim_cutoff)\n",
        "            nodes = postprocessor.postprocess_nodes(nodes)\n",
        "        return nodes\n",
        "\n",
        "    def _node_to_text(self, nodes: TextNode, return_too_long: bool = False,):\n",
        "        \"\"\"\n",
        "        Intakes llamaindex nodes and returns the associated text along with number of words and\n",
        "        whether or not the number of words may exceed the context window of Gemini.\n",
        "        \"\"\"\n",
        "        total_len = 0\n",
        "        texts = []\n",
        "        for n in nodes:\n",
        "            txt = n.text\n",
        "            total_len += len(txt.split(\" \"))\n",
        "            texts.append(txt)\n",
        "        if (\n",
        "            ((\"flash\" in self.model) & (total_len > 300_000))\n",
        "            |\n",
        "            ((\"pro\" in self.model) & (total_len > 1_000_000))\n",
        "            ):\n",
        "            too_long = True\n",
        "        else:\n",
        "            too_long = False\n",
        "            return texts, too_long, total_len\n",
        "        return texts\n",
        "\n",
        "    def _retrieve_original_doc(self, nodes: TextNode):\n",
        "        \"Get the original law text, not the summaries.\"\n",
        "        total_len = 0\n",
        "        docs = []\n",
        "        indices = [n.metadata['index_id'] for n in nodes]\n",
        "        for d in self.law_docs:\n",
        "            if d.metadata['index_id'] in indices:\n",
        "                txt = d.text\n",
        "                total_len += len(txt.split(\" \"))\n",
        "                docs.append(txt)\n",
        "        if (\n",
        "            ((\"flash\" in self.model) & (total_len > 300_000))\n",
        "            |\n",
        "            ((\"pro\" in self.model) & (total_len > 1_000_000))\n",
        "            ):\n",
        "            too_long = True\n",
        "        else:\n",
        "            too_long = False\n",
        "\n",
        "        return docs, total_len, too_long\n",
        "\n",
        "    def _reset_chat(self):\n",
        "\n",
        "        self.chat = None\n",
        "        self.chat_len = None\n",
        "        self.chat_start_time = None\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T22:04:37.053049Z",
          "iopub.execute_input": "2024-12-01T22:04:37.053476Z",
          "iopub.status.idle": "2024-12-01T22:04:37.076794Z",
          "shell.execute_reply.started": "2024-12-01T22:04:37.053438Z",
          "shell.execute_reply": "2024-12-01T22:04:37.07537Z"
        },
        "id": "rDcgdF9V6Y9J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Individual Concerned About Medicare"
      ],
      "metadata": {
        "id": "Y3YWPLjD6Y9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "law_bot = LegislatureAgent(\n",
        "    model=\"flash\",\n",
        "    legislature_index=passed_laws_index,\n",
        "    summary_index=summarized_law_index,\n",
        "    vector_store_info=vector_store_info,\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T22:04:39.159209Z",
          "iopub.execute_input": "2024-12-01T22:04:39.15959Z",
          "iopub.status.idle": "2024-12-01T22:04:39.281149Z",
          "shell.execute_reply.started": "2024-12-01T22:04:39.159552Z",
          "shell.execute_reply": "2024-12-01T22:04:39.280152Z"
        },
        "id": "Joyb5RfG6Y9J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets start by asking for a summary of medicare laws."
      ],
      "metadata": {
        "id": "lK8H3T1T6Y9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "law_bot.summary(\n",
        "    about=\"healthcare impacting medicare recipients.\",\n",
        "    chat_len=15,\n",
        "    temp=0.1,\n",
        "    num_sentences=5,\n",
        "    sim_cutoff=0.4\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:28.741959Z",
          "iopub.execute_input": "2024-12-01T22:00:28.742731Z",
          "iopub.status.idle": "2024-12-01T22:00:33.976815Z",
          "shell.execute_reply.started": "2024-12-01T22:00:28.74269Z",
          "shell.execute_reply": "2024-12-01T22:00:33.975775Z"
        },
        "id": "FwOi8o9r6Y9J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now lets try and start a chat with context caching."
      ],
      "metadata": {
        "id": "a6kPbVC46Y9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "law_bot.start_chat(\n",
        "    about=\"healthcare impacting medicare recipients.\",\n",
        "    chat_len=30, #minutes\n",
        "    temp=0.1,\n",
        "    sim_cutoff=0.4\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T22:04:41.423144Z",
          "iopub.execute_input": "2024-12-01T22:04:41.423538Z",
          "iopub.status.idle": "2024-12-01T22:04:44.973979Z",
          "shell.execute_reply.started": "2024-12-01T22:04:41.423503Z",
          "shell.execute_reply": "2024-12-01T22:04:44.972949Z"
        },
        "id": "wmxjjrtJ6Y9J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "law_bot.ask(\"Will changes to medicare law impact recipients negatively in the next year?\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T22:06:05.275211Z",
          "iopub.execute_input": "2024-12-01T22:06:05.275605Z",
          "iopub.status.idle": "2024-12-01T22:06:16.099702Z",
          "shell.execute_reply.started": "2024-12-01T22:06:05.275571Z",
          "shell.execute_reply": "2024-12-01T22:06:16.098706Z"
        },
        "id": "o4dsR2-v6Y9J"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}